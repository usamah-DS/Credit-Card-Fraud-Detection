---
title: "Credit Card Fraud Detection"
author: "Usama"
date: "2024-10-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this project, I aim to build machine learning models that detect fraudulent credit card transactions using a dataset of transactions made by European cardholders. The dataset is highly imbalanced, with a very small percentage of transactions being fraudulent. I will apply **Logistic Regression** and **Decision Tree** models to classify the transactions and evaluate the models' performance using various metrics such as accuracy, precision, recall, and ROC-AUC.

## Methods

### Data Loading and Exploration

I first load and explore the dataset. I examine the class distribution to highlight the class imbalance problem and check for missing values.

```{r}
# Load required libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ROSE)) install.packages("ROSE", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org") # For decision tree

library(tidyverse)
library(caret)
library(ROSE)
library(pROC)
library(rpart)
# Load the dataset and inspect its structure
credit_data <- read.csv("C:/Users/usama/Downloads/Credit Card Fraud Detection/creditcard.csv")
str(credit_data)
summary(credit_data)

# Check for missing values
sum(is.na(credit_data))

```

### Handling Class Imbalance

Since the dataset is highly imbalanced, with only 0.17% of the transactions being fraudulent, I apply the ROSE technique to balance the data.

```{r}
# Apply ROSE to balance the dataset
balanced_data <- ROSE(Class ~ ., data = credit_data, seed = 1)$data

# Check the new class distribution
balanced_data %>%
  group_by(Class) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = as.factor(Class), y = count, fill = as.factor(Class))) +
  geom_bar(stat = "identity") +
  labs(title = "Balanced Class Distribution", x = "Class", y = "Count")

```

### Splitting the Data into Training and Test Sets

I split the data into **training** and **testing** sets (80% training, 20% testing) to ensure that our models can be evaluated effectively.

```{r}
# Split the data into training and testing sets (80% training, 20% testing)
set.seed(123)
train_index <- createDataPartition(balanced_data$Class, p = 0.8, list = FALSE)
train_data <- balanced_data[train_index, ]
test_data <- balanced_data[-train_index, ]

# Verify the split
table(train_data$Class)
table(test_data$Class)

```

### Modeling

I apply **Logistic Regression** and **Decision Tree** models to the dataset.

#### Logistic Regression

```{r}
# Logistic Regression Model
log_model <- glm(Class ~ ., data = train_data, family = binomial)

# Predictions
log_preds <- predict(log_model, test_data, type = "response")
log_preds_class <- ifelse(log_preds > 0.5, 1, 0)

# Confusion Matrix for Logistic Regression
log_cm <- confusionMatrix(as.factor(log_preds_class), as.factor(test_data$Class))
log_cm

```

#### Decision Tree

```{r}
# Decision Tree Model
tree_model <- rpart(Class ~ ., data = train_data, method = "class")

# Predictions
tree_preds <- predict(tree_model, test_data, type = "class")

# Confusion Matrix for Decision Tree
tree_cm <- confusionMatrix(tree_preds, as.factor(test_data$Class))
tree_cm

```

### Evaluation: ROC Curves

I evaluate both models using ROC-AUC curves to compare their performance.

```{r}
# ROC Curve for Logistic Regression
roc_curve_log <- roc(test_data$Class, log_preds)
plot(roc_curve_log, col = "blue", main = "ROC Curve for Logistic Regression")

# ROC Curve for Decision Tree
tree_probs <- predict(tree_model, test_data, type = "prob")[, 2]
roc_curve_tree <- roc(test_data$Class, tree_probs)
plot(roc_curve_tree, col = "red", add = TRUE)

# Display AUC values
log_auc <- auc(roc_curve_log)
tree_auc <- auc(roc_curve_tree)
cat("Logistic Regression AUC:", log_auc, "\n")
cat("Decision Tree AUC:", tree_auc, "\n")

```

## Results

The table below summarizes the confusion matrices and AUC values for both **Logistic Regression** and **Decision Tree** models:

| Model | Accuracy | Precision | Recall | F1-Score | AUC |
|----|----|----|----|----|----|
| Logistic Regression | `r log_cm$overall['Accuracy']` | `r log_cm$byClass['Precision']` | `r log_cm$byClass['Recall']` | `r log_cm$byClass['F1']` | `r log_auc` |
| Decision Tree | `r tree_cm$overall['Accuracy']` | `r tree_cm$byClass['Precision']` | `r tree_cm$byClass['Recall']` | `r tree_cm$byClass['F1']` | `r tree_auc` |

## Conclusion

In this project, I applied both **Logistic Regression** and **Decision Tree** models to detect fraudulent credit card transactions. Both models performed well, and further improvements can be made by experimenting with additional algorithms or improving feature engineering.

## References

-   UCI Machine Learning Repository: [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data)
